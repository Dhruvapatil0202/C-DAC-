{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c76ef3dc-2992-4cd8-bc84-8bf9efeb9256",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries for NLP\n",
    "# 1. nltk -> natural lang. toolkit\n",
    "# 2. spacy \n",
    "# 3. textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a881073-a722-40e5-ab1b-0b7f8c711080",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "\n",
    "processes in beginning of NLP\n",
    "1. Data cleaning\n",
    "2. Data Encoding\n",
    "3. Normalization\n",
    "4. Feature selection\n",
    "5. Cross validation\n",
    "\n",
    "why preprocessing\n",
    "\n",
    "1. significance of text preprocessing in the performance of models.\n",
    "2. Data preprocessing is an essential step in buildign a machine learning model and depending on how well the data has been preprocessed; the results are seen\n",
    "3. In NLP, text preprocessing is the first step in the process of building a model.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acb8a7cb-37dd-4198-84a9-ce39a07e1e8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nltk spacy textblob -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a55ac5cd-c73b-4c87-b37f-23a4cb7490d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import textblob\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0334aa4b-3f99-49c0-96fd-9c21f478764c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "showing info https://raw.githubusercontent.com/nltk/nltk_data/gh-pages/index.xml\n"
     ]
    }
   ],
   "source": [
    "# nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e5aa2d8-34cd-4256-a333-5bdfa34a1900",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\DAI.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\DAI.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt_tab.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\DAI.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\DAI.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\DAI.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\DAI.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package indian to\n",
      "[nltk_data]     C:\\Users\\DAI.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package indian is already up-to-date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\DAI.STUDENTSDC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download(\"punkt\") # Tokenization\n",
    "nltk.download(\"punkt_tab\")\n",
    "nltk.download(\"stopwords\") # Collection of stopwords\n",
    "nltk.download(\"wordnet\") # Wordnet -> databse of english words\n",
    "nltk.download(\"omw-1.4\") # Open multilinguial wordnet\n",
    "nltk.download(\"averaged_perceptron_tagger\") # POS tagging\n",
    "nltk.download(\"indian\") # Indian language POS tagging\n",
    "nltk.download(\"maxent_ne_chunker\") # NER tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0daed7de-65a9-44d5-80d3-2a07c123ba46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nTokenization:\\n\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Tokenization:\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "71690d39-3495-4d08-9bea-a0c124c8661f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They',\n",
       " 'told',\n",
       " 'that',\n",
       " 'their',\n",
       " 'ages',\n",
       " 'are',\n",
       " '25,',\n",
       " '26',\n",
       " 'and',\n",
       " '31',\n",
       " 'respectively.']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent = \"They told that their ages are 25, 26 and 31 respectively.\"\n",
    "sent.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7b8a109d-7ee4-41f1-830c-c475e1809d1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['They',\n",
       " 'told',\n",
       " 'that',\n",
       " 'their',\n",
       " 'ages',\n",
       " 'are',\n",
       " '25',\n",
       " ',',\n",
       " '26',\n",
       " 'and',\n",
       " '31',\n",
       " 'respectively',\n",
       " '.']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "word_tokenize(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c219c1cc-00d4-4cb0-adea-685f12cec1c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Dr.', 'Deshmukh', 'told', 'me', 'about', 'this', '.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"Dr. Deshmukh told me about this.\"\n",
    "word_tokenize(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1f29e95b-9814-4c57-b98b-b8bc0c409c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sents = \"\"\"Hello friends!\n",
    "How are you? Welcome to the world of Python Programming.\"\"\"\n",
    "# What are percentage of punctuation -> 3 / 15 => 20% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "0a627597-5dec-479f-9a24-212f1cf54010",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = word_tokenize(sents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4d844b45-c6d1-49af-b79d-23a8dae97550",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20.0"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "puncts = [token for token in tokens if token in string.punctuation]\n",
    "(len(puncts) / len(tokens)) * 100"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (surprise)",
   "language": "python",
   "name": "surprise"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
