# Intialization
import os
import sys

os.environ["SPARK_HOME"] = "/home/talentum/spark"
os.environ["PYLIB"] = os.environ["SPARK_HOME"] + "/python/lib"
# In below two lines, use /usr/bin/python2.7 if you want to use Python 2
os.environ["PYSPARK_PYTHON"] = "/usr/bin/python3.6" 
os.environ["PYSPARK_DRIVER_PYTHON"] = "/usr/bin/python3"
sys.path.insert(0, os.environ["PYLIB"] +"/py4j-0.10.7-src.zip")
sys.path.insert(0, os.environ["PYLIB"] +"/pyspark.zip")

# NOTE: Whichever package you want mention here.
# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' 
# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'
os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'
# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'

#Entrypoint 2.x
from pyspark.sql import SparkSession
spark = SparkSession.builder.appName("Spark SQL basic example").enableHiveSupport().getOrCreate()

# On yarn:
# spark = SparkSession.builder.appName("Spark SQL basic example").enableHiveSupport().master("yarn").getOrCreate()
# specify .master("yarn")

sc = spark.sparkContext

____________________________________________________



1. RDD to dataFrame 
spark_df = spark.createDataFrame(rdd, schema = ["name", "age"])

2. CSV to dataFrame
spark_df = spark.read.csv(file_path, header = True, inferSchema = True)

___________________________________________________

spark dataframe

1. view schema
df.printSchema()

2. show sample data
df.show(5) show first 5 rows

3. count number of rows 
df.count()

4. select specific column
df.select("col1", "col2")

5.filter rows
df.filter(df['col1'] > 10)

6. group by and aggregate
df.groupBy("col1").agg({"col1" : "mean"})

7. sort data
df.sort(df["col1"].desc())

8. add new col
df.withColumn("new_col", df["col1"] * 2)

9. drop column
df.drop("col1")

10. dataframe to csv
df.write.csv("out_path", header = True)