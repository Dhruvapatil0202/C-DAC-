{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7db297df",
   "metadata": {},
   "source": [
    "# Interactive Use of PySpark\n",
    "\n",
    "- Spark comes with an interactive python shell in which PySpark is already installed in it. PySpark shell is useful for basic testing and debugging and it is quite powerful. The easiest way to demonstrate the power of PySparkâ€™s shell is to start using it. In this example, you'll load a simple list containing numbers ranging from 1 to 100 in the PySpark shell.\n",
    "\n",
    "- The most important thing to understand here is that we are not creating any `SparkContext` object because PySpark automatically creates the `SparkContext` object named `sc`, by default in the PySpark shell."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd59a35f",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "- Create a python list named `numb` containing the numbers 1 to 100.\n",
    "- Load the list into Spark using Spark Context's `parallelize` method and assign it to a variable `spark_data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "24bc5dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6cc69a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5b342b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a python list of numbers from 1 to 100 \n",
    "numb = range(1, 101)\n",
    "\n",
    "# Load the list into PySpark  \n",
    "spark_data = sc.parallelize(numb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d49af68c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83, 84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99, 100]\n"
     ]
    }
   ],
   "source": [
    "print(spark_data.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "971da105",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark_data.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c576ec31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on method parallelize in module pyspark.context:\n",
      "\n",
      "parallelize(c, numSlices=None) method of pyspark.context.SparkContext instance\n",
      "    Distribute a local Python collection to form an RDD. Using xrange\n",
      "    is recommended if the input represents a range for performance.\n",
      "    \n",
      "    >>> sc.parallelize([0, 2, 3, 4, 6], 5).glom().collect()\n",
      "    [[0], [2], [3], [4], [6]]\n",
      "    >>> sc.parallelize(xrange(0, 6, 2), 5).glom().collect()\n",
      "    [[], [0], [], [2], [4]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(sc.parallelize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6527b514",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
