{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb78dcdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Intialization\n",
    "import os\n",
    "import sys\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = \"/home/talentum/spark\"\n",
    "os.environ[\"PYLIB\"] = os.environ[\"SPARK_HOME\"] + \"/python/lib\"\n",
    "# In below two lines, use /usr/bin/python2.7 if you want to use Python 2\n",
    "os.environ[\"PYSPARK_PYTHON\"] = \"/usr/bin/python3.6\" \n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = \"/usr/bin/python3\"\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/py4j-0.10.7-src.zip\")\n",
    "sys.path.insert(0, os.environ[\"PYLIB\"] +\"/pyspark.zip\")\n",
    "\n",
    "# NOTE: Whichever package you want mention here.\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0 pyspark-shell' \n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'\n",
    "os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.3 pyspark-shell'\n",
    "# os.environ['PYSPARK_SUBMIT_ARGS'] = '--packages com.databricks:spark-xml_2.11:0.6.0,org.apache.spark:spark-avro_2.11:2.4.0 pyspark-shell'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6a6e8a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Entrypoint 2.x\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().getOrCreate()\n",
    "\n",
    "# On yarn:\n",
    "# spark = SparkSession.builder.appName(\"Spark SQL basic example\").enableHiveSupport().master(\"yarn\").getOrCreate()\n",
    "# specify .master(\"yarn\")\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1c31cdee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "apache --> 1\n",
      "spark --> 15\n",
      "is --> 7\n",
      "a --> 10\n",
      "fast --> 1\n",
      "and --> 11\n",
      "general --> 3\n",
      "cluster --> 2\n",
      "computing --> 1\n",
      "system --> 1\n",
      "for --> 15\n",
      "big --> 1\n",
      "it --> 2\n",
      "provides --> 1\n",
      "apis --> 1\n",
      "in --> 5\n",
      "an --> 4\n",
      "optimized --> 1\n",
      "engine --> 1\n",
      "that --> 2\n",
      "supports --> 2\n",
      "computation --> 1\n",
      "graphs --> 1\n",
      "data --> 1\n",
      "also --> 5\n",
      "rich --> 1\n",
      "set --> 2\n",
      "of --> 5\n",
      "tools --> 1\n",
      "including --> 4\n",
      "sql --> 2\n",
      "mllib --> 1\n",
      "machine --> 1\n",
      "graphx --> 1\n",
      "graph --> 1\n",
      "streaming --> 1\n",
      "stream --> 1\n",
      "online --> 2\n",
      "documentation --> 4\n",
      "you --> 7\n",
      "can --> 6\n",
      "find --> 1\n",
      "the --> 24\n",
      "latest --> 1\n",
      "programming --> 1\n",
      "on --> 7\n",
      "web --> 1\n",
      "this --> 3\n",
      "readme --> 1\n",
      "file --> 1\n",
      "only --> 1\n",
      "contains --> 1\n",
      "basic --> 1\n",
      "setup --> 1\n",
      "building --> 3\n",
      "built --> 1\n",
      "using --> 3\n",
      "to --> 19\n",
      "build --> 3\n",
      "its --> 1\n",
      "example --> 4\n",
      "clean --> 1\n",
      "package --> 1\n",
      "do --> 2\n",
      "not --> 1\n",
      "need --> 1\n",
      "if --> 4\n",
      "downloaded --> 1\n",
      "more --> 1\n",
      "detailed --> 2\n",
      "available --> 1\n",
      "from --> 1\n",
      "project --> 1\n",
      "at --> 2\n",
      "development --> 1\n",
      "info --> 1\n",
      "developing --> 1\n",
      "see --> 3\n",
      "developer --> 1\n",
      "interactive --> 2\n",
      "scala --> 2\n",
      "shell --> 2\n",
      "easiest --> 1\n",
      "way --> 1\n",
      "start --> 1\n",
      "through --> 1\n",
      "try --> 1\n",
      "following --> 2\n",
      "which --> 2\n",
      "should --> 2\n",
      "return --> 2\n",
      "python --> 2\n",
      "prefer --> 1\n",
      "use --> 3\n",
      "run --> 7\n",
      "programs --> 3\n",
      "comes --> 1\n",
      "with --> 3\n",
      "several --> 1\n",
      "sample --> 1\n",
      "one --> 2\n",
      "sparkpi --> 2\n",
      "will --> 1\n",
      "pi --> 1\n",
      "master --> 1\n",
      "environment --> 1\n",
      "variable --> 1\n",
      "when --> 1\n",
      "running --> 2\n",
      "examples --> 2\n",
      "submit --> 1\n",
      "be --> 2\n",
      "or --> 3\n",
      "locally --> 2\n",
      "n --> 1\n",
      "abbreviated --> 1\n",
      "class --> 2\n",
      "name --> 1\n",
      "many --> 1\n",
      "print --> 1\n",
      "usage --> 1\n",
      "help --> 1\n",
      "no --> 1\n",
      "params --> 1\n",
      "are --> 1\n",
      "tests --> 3\n",
      "testing --> 1\n",
      "first --> 1\n",
      "requires --> 1\n",
      "once --> 1\n",
      "please --> 4\n",
      "guidance --> 2\n",
      "how --> 3\n",
      "individual --> 1\n",
      "there --> 1\n",
      "kubernetes --> 1\n",
      "integration --> 1\n",
      "note --> 1\n",
      "about --> 1\n",
      "hadoop --> 3\n",
      "versions --> 2\n",
      "uses --> 1\n",
      "core --> 1\n",
      "library --> 1\n",
      "talk --> 1\n",
      "hdfs --> 1\n",
      "other --> 1\n",
      "storage --> 1\n",
      "because --> 1\n",
      "protocols --> 1\n",
      "have --> 1\n",
      "changed --> 1\n",
      "different --> 1\n",
      "must --> 1\n",
      "against --> 1\n",
      "same --> 1\n",
      "version --> 2\n",
      "your --> 1\n",
      "refer --> 2\n",
      "enabling --> 1\n",
      "particular --> 2\n",
      "distribution --> 1\n",
      "hive --> 2\n",
      "thriftserver --> 1\n",
      "configuration --> 1\n",
      "overview --> 1\n",
      "configure --> 1\n",
      "contributing --> 2\n",
      "review --> 1\n",
      "information --> 1\n",
      "get --> 1\n",
      "started --> 1\n"
     ]
    }
   ],
   "source": [
    "file = \"file:////home/talentum/spark/README.md\"\n",
    "# import \n",
    "\n",
    "RDD_1 = sc.textFile(file\n",
    "                   ).flatMap(lambda x: x.split(\" \")\n",
    "                   ).filter(lambda x: x.isalpha()\n",
    "                   ).map(lambda x: (x.lower(), 1)\n",
    "                   ).reduceByKey(lambda x, y: x + y)\n",
    "    \n",
    "for word, freq in RDD_1.collect():\n",
    "    print(word, \"-->\", freq)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
